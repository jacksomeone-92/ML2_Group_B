{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Group B - Counter Strike GO round winner classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: ALBIN S., CKALIB N., DANIEL R., JORGE ANDRÉS R., NICOLAS G., TOMÁS F.\n",
    "\n",
    "Data source: https://www.kaggle.com/christianlillelund/csgo-round-winner-classification\n",
    "\n",
    "Last revision: 13/September/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/CounterStrikeGO.jpg\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "YrnrAFLgZDSw"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "We're putting in practice what we need to know about feature engineering and model validation.\n",
    "\n",
    "CS:GO is a tactical shooter, where two teams (CT and Terrorist) play for a best of 30 rounds, with each round being 1 minute and 55 seconds. There are 5 players on each team (10 in total) and the first team to reach 16 rounds wins the game. At the start, one team plays as CT and the other as Terrorist. After 15 rounds played, the teams swap side. There are 7 different maps a game can be played on. You win a round as Terrorist by either planting the bomb and making sure it explodes, or by eliminating the other team. You win a round as CT by either eliminating the other team, or by disarming the bomb, should it have been planted.\n",
    "\n",
    "Learn more about CS:GO: https://en.wikipedia.org/wiki/Counter-Strike:_Global_Offensive\n",
    "\n",
    "The dataset consists of ~700 demos from high level tournament play in 2019 and 2020. Warmup rounds and restarts have been filtered, and for the remaining live rounds a round snapshot have been recorded every 20 seconds until the round is decided. The target variable is the one called `round_winner`, taking the values `CT` for Counter-Terrorist or `T` for Terrorist.\n",
    "\n",
    "## Steps to be followed <font color=red>[CAN BE REMOVED ONCE WE FINISH]</font>\n",
    "\n",
    "1. Read the dataset\n",
    "2. Data preparation: Check for variable types, NAs, values imputation, column names, scaling, encoding, etc.\n",
    "3. EDA (Exploratory Data Analysis): Try to extract some insights from the data like features completely uncorrelated or under-represented. Can we merge values from any of the categorical features? Would it be beneficial to discretize numerical features? Do we have outliers?\n",
    "3. Baseline: Simply take the simpler possible model (logistic regression) and set a base score that we'll try to improve along the process. To run logistic regression **you need** to have **numerical features**, so the fastest way of preparing your data to be used in `lr` is to perform _one hot encoding_. Perform this encoding so that you don't destroy the original prepared data. Consider to include onehot encoding as an step which is done right before evaluation over a copy of your prepared data.\n",
    "4. Feature Engineering: We will try\n",
    "    - categorical encoding: compare techniques like onehot and target encoding\n",
    "    - feature selection: compare the results from filtering, wrappers and regularization\n",
    "    - feature construction: compare GPLearn with _ad hoc_ methods, or Deep Feature Synthesis.\n",
    "5. Evaluation: the goal here is to fine tune our models, so, to do that we need a new model, like **decission trees**, in this case. We will experiment with:\n",
    "    - Cross validation and bootstrapping comparison.\n",
    "    - Fine tune tree parameters: pruning and parameters optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "salUSFPkZDSx"
   },
   "source": [
    "# Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "QF0S-mm7ZDSy"
   },
   "outputs": [],
   "source": [
    "# import libraries numpy, pandas, and scipy.stats module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JhJjYdq1ZiIw"
   },
   "outputs": [],
   "source": [
    "# import dataset using pd.read_csv() function\n",
    "df = pd.read_csv(\"modeling_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p8t1dViWZDS3"
   },
   "source": [
    "Check imports needed and place all at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "LwYEAvn5ZDS4",
    "outputId": "e4ddff26-ef53-4fd5-8a78-2b0487450123"
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.utils import resample\n",
    "from sklearn_pandas import CategoricalImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "fTRl2boJZDS7"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "rKcgywXRZDS8",
    "outputId": "915484d7-65a3-4653-8d7a-02256f6a840b"
   },
   "outputs": [],
   "source": [
    "# Our target feature is 'round_winner', replace the labels\n",
    "# 'CT' and 'T' in the data for '1' and '0'\n",
    "target = 'round_winner'\n",
    "df[target] = df[target].replace({'CT': 1, 'T':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "ojbjIJHbZDS-"
   },
   "source": [
    "# Data Preparation <font color=red>ALBIN AND JORGE</font>\n",
    "\n",
    "Check for NA, not relevant variables, outliers, scale numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "SD8HONykZDTB"
   },
   "source": [
    "WHAT to do with NA's. Then, scale numerical variables, and finally check if all categorical variables are correctly encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "vWCPc5gCZDTC",
    "outputId": "3d4919b0-f405-4b1c-a390-15338231f6c0"
   },
   "outputs": [],
   "source": [
    "# Get the number of NAs per feature\n",
    "num_nas = []\n",
    "for feature in df.columns:\n",
    "    num_nas.append(df[feature].isna().sum())\n",
    "print(f'{len(df.columns)} features have a median of {math.floor(np.median(num_nas))} NAs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "J7G9F6fjZDTE"
   },
   "source": [
    "## Imputation and Scaling <font color=red>Imputation is not needed as it seems we have no NA</font>\n",
    "\n",
    "There are [several ways of imputing missing values](https://scikit-learn.org/stable/modules/impute.html#multivariate-feature-imputation). I'd like to use a multivariate approach, which is much better than univariate methods (imputing by the mean, etc.). To do so, I will use the methods available in `scikit-learn` library ([here](https://scikit-learn.org/stable/modules/impute.html#multivariate-feature-imputation)).\n",
    "\n",
    "<font color=red>Could be a good idea to split categorical and numerical features to work separately on them</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "hide_input": false,
    "id": "5Hv5DS0tZDTF"
   },
   "outputs": [],
   "source": [
    "num_imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "num_feats = num_imputer.fit_transform(df.features[data.numerical_features].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3apyh7kZDTH"
   },
   "source": [
    "To perform imputaton on categorical variables, I decided to use a method provided by the library `sklearn_pandas` that implements a `CategoricalImputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPmlkl0tZDTI"
   },
   "outputs": [],
   "source": [
    "cat_imputer = CategoricalImputer()\n",
    "df = pd.DataFrame()\n",
    "for col in data.categorical_features_na:\n",
    "    df[col] = cat_imputer.fit_transform(data.features[col].values)\n",
    "ds = Dataset.from_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ngpfTkniZDTK"
   },
   "source": [
    "Finally, the result of imputing both: numerical and categorical, is now merged together in a single Dataset (or you can also use a dataframe, if you prefer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "hide_input": false,
    "id": "VSa9CBBxZDTL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds.add_columns(data.target)\n",
    "ds.set_target('band_type')\n",
    "\n",
    "# add the resulting imputed numerical features to the categorical imputed\n",
    "ds.add_columns(pd.DataFrame(num_feats, columns=data.numerical_features));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rEODkPPZDTN"
   },
   "source": [
    "Scale all the numeric values, at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J32xn7ARZDTN"
   },
   "outputs": [],
   "source": [
    "#Scale numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "flVqyT5bZDTP"
   },
   "source": [
    "Now, my dataset is ready to be analyzed. Lots of categorical and numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "-m3bwfd7ZDTP"
   },
   "source": [
    "# EDA <font color=red>NICOLAS AND DANIEL</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "colab_type": "code",
    "id": "NYwga4OpZDTQ",
    "outputId": "c4bfed14-d231-4485-c753-784828c907a0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "agscaiiaZDTS"
   },
   "source": [
    "I also want to know how may samples belong to the class '1' and how many to class '0', simply to check if the data set is balanced (you can see that YES, it is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "Ydw4MnThZDTS",
    "outputId": "dd3089bd-73c8-436a-c918-92986a26fc31"
   },
   "outputs": [],
   "source": [
    "df[target].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "fTDXjtOnZDTU"
   },
   "source": [
    "Starting by knowing if there're underrepresented features: this is, values that are only present in a very small portion of the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "SdNmJCMeZDTU",
    "outputId": "affdaef1-8223-4beb-a922-841230a9db25"
   },
   "outputs": [],
   "source": [
    "#code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3F5bIZQfZDTW"
   },
   "source": [
    "Let's see what's the situation in the categories of those clases. I will simply count the number of unique values present at each of those columns (above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "GW-DIJsbZDTW",
    "outputId": "9c54959e-9def-4b6d-8df6-6131bb7627e7"
   },
   "outputs": [],
   "source": [
    "for col in df:\n",
    "    print(col)\n",
    "    res = df[col].value_counts()\n",
    "    print(f'{res}\\n----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fa9XrhgpZDTY"
   },
   "source": [
    "From that result, we will remove those features, as they are not very informative (a priori). You can also combine them into a single feature by merging the four of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQROEA2jZDTY"
   },
   "outputs": [],
   "source": [
    "#Merge categories if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SC6PLepUZDTc"
   },
   "source": [
    "Let's take a look at the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bokeh libraries\n",
    "from bokeh.io import output_file, output_notebook\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.layouts import row, column, gridplot\n",
    "from bokeh.models.widgets import Tabs, Panel\n",
    "from bokeh.models import ColumnDataSource, HoverTool, Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BokehHistogram():\n",
    "    '''\n",
    "    A class to simplify the making of interactive histograms with the Bokeh library.\n",
    "    Requires: Bokeh, Pandas, and Numpy.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, colors=[\"SteelBlue\", \"Tan\"], height=600, width=600):\n",
    "        self.colors = colors\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def hist_hover(self, dataframe, column, bins=30, log_scale=False, show_plot=True):\n",
    "        \"\"\"\n",
    "        A method for creating a sinlge Bokeh histogram with hovertool interactivity.\n",
    "        Parameters:\n",
    "        ----------\n",
    "        Input:\n",
    "        dataframe {df}: Pandas dataframe\n",
    "        column {string}: column of dataframe to plot in histogram\n",
    "        bins {int}: number of bins in histogram\n",
    "        log_scale {bool}: True to plot on a log scale\n",
    "        colors {list -> string}: list of colors for histogram; first color default color, second color is hover color\n",
    "        show_plot {bool}: True to display the plot, False to store the plot in a variable (for use in later methods)\n",
    "        Output:\n",
    "        plot: bokeh historgram with interactive hover tool\n",
    "        \"\"\"\n",
    "        # build histogram data with Numpy\n",
    "        hist, edges = np.histogram(dataframe[column], bins = bins)\n",
    "        hist_df = pd.DataFrame({column: hist,\n",
    "                                 \"left\": edges[:-1],\n",
    "                                 \"right\": edges[1:]})\n",
    "        hist_df[\"interval\"] = [\"%d to %d\" % (left, right) for left, \n",
    "                               right in zip(hist_df[\"left\"], hist_df[\"right\"])]\n",
    "        # bokeh histogram with hover tool\n",
    "        if log_scale == True:\n",
    "            hist_df[\"log\"] = np.log(hist_df[column])\n",
    "            src = ColumnDataSource(hist_df)\n",
    "            plot = figure(plot_height = self.height, plot_width = self.width,\n",
    "                  title = \"Histogram of {}\".format(column.capitalize()),\n",
    "                  x_axis_label = column.capitalize(),\n",
    "                  y_axis_label = \"Log Count\")    \n",
    "            plot.quad(bottom = 0, top = \"log\",left = \"left\", \n",
    "                right = \"right\", source = src, fill_color = self.colors[0], \n",
    "                line_color = \"black\", fill_alpha = 0.7,\n",
    "                hover_fill_alpha = 1.0, hover_fill_color = self.colors[1])\n",
    "        else:\n",
    "            src = ColumnDataSource(hist_df)\n",
    "            plot = figure(plot_height = self.height, plot_width = self.width,\n",
    "                  title = \"Histogram of {}\".format(column.capitalize()),\n",
    "                  x_axis_label = column.capitalize(),\n",
    "                  y_axis_label = \"Count\")    \n",
    "            plot.quad(bottom = 0, top = column,left = \"left\", \n",
    "                right = \"right\", source = src, fill_color = self.colors[0], \n",
    "                line_color = \"black\", fill_alpha = 0.7,\n",
    "                hover_fill_alpha = 1.0, hover_fill_color = self.colors[1])\n",
    "\n",
    "        # hover tool\n",
    "        hover = HoverTool(tooltips = [('Interval', '@interval'),\n",
    "                                  ('Count', str(\"@\" + column))])\n",
    "        plot.add_tools(hover)\n",
    "\n",
    "        # output\n",
    "        if show_plot == True:\n",
    "            show(plot)\n",
    "        else:\n",
    "            return plot\n",
    "\n",
    "    def histotabs(self, dataframe, features, log_scale=False, show_plot=False):\n",
    "        '''\n",
    "        Builds tabbed interface for a series of histograms; calls hist_hover. Specifying 'show_plot=True' will simply display the histograms in sequence rather than in a tabbed interface.\n",
    "        Parameters:\n",
    "        ----------\n",
    "        Input:\n",
    "        dataframe {df}: a Pandas dataframe\n",
    "        features {list -> string}: list of features to plot\n",
    "        log_scale {bool}: True to plot on a log scale\n",
    "        colors {list -> string}: list of colors for histogram; first color default color, second color is hover color\n",
    "        show_plot {bool}: True to display the plot, False to store the plot in a variable (for use in later methods)\n",
    "        Output:\n",
    "        Tabbed interface for viewing interactive histograms of specified features\n",
    "        '''\n",
    "        hists = []\n",
    "        for f in features:\n",
    "            h = self.hist_hover(dataframe, f, log_scale=log_scale, show_plot=show_plot)\n",
    "            p = Panel(child=h, title=f.capitalize())\n",
    "            hists.append(p)\n",
    "        t = Tabs(tabs=hists)\n",
    "        show(t)\n",
    "\n",
    "    def filtered_histotabs(self, dataframe, feature, filter_feature, log_scale=False, show_plot=False):\n",
    "        '''\n",
    "        Builds tabbed histogram interface for one feature filtered by another. Feature is numeric, fiter feature is categorical.\n",
    "        Parameters:\n",
    "        ----------\n",
    "        Input:\n",
    "        dataframe {df}: a Pandas dataframe\n",
    "        features {list -> string}: list of features to plot\n",
    "        log_scale {bool}: True to plot on a log scale\n",
    "        colors {list -> string}: list of colors for histogram; first color default color, second color is hover color\n",
    "        show_plot {bool}: True to display the plot, False to store the plot in a variable (for use in later methods)\n",
    "        Output:\n",
    "        Tabbed interface for viewing interactive histograms of specified feature filtered by categorical filter feature\n",
    "        '''\n",
    "        hists = []\n",
    "        for col in dataframe[filter_feature].unique():\n",
    "            sub_df = dataframe[dataframe[filter_feature] == col]\n",
    "            histo = self.hist_hover(sub_df, feature, log_scale=log_scale, show_plot=show_plot)\n",
    "            p = Panel(child = histo, title=col)\n",
    "            hists.append(p)\n",
    "        t = Tabs(tabs=hists)\n",
    "        show(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of the bokeeh histogram object\n",
    "h = BokehHistogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time_left',\n",
       " 'ct_score',\n",
       " 't_score',\n",
       " 'map',\n",
       " 'bomb_planted',\n",
       " 'ct_health',\n",
       " 't_health',\n",
       " 'ct_armor',\n",
       " 't_armor',\n",
       " 'ct_money',\n",
       " 't_money',\n",
       " 'ct_helmets',\n",
       " 't_helmets',\n",
       " 'ct_defuse_kits',\n",
       " 'ct_players_alive',\n",
       " 't_players_alive',\n",
       " 'ct_weapon_ak47',\n",
       " 't_weapon_ak47',\n",
       " 'ct_weapon_aug',\n",
       " 't_weapon_aug',\n",
       " 'ct_weapon_awp',\n",
       " 't_weapon_awp',\n",
       " 'ct_weapon_bizon',\n",
       " 't_weapon_bizon',\n",
       " 'ct_weapon_cz75auto',\n",
       " 't_weapon_cz75auto',\n",
       " 'ct_weapon_elite',\n",
       " 't_weapon_elite',\n",
       " 'ct_weapon_famas',\n",
       " 't_weapon_famas',\n",
       " 'ct_weapon_g3sg1',\n",
       " 't_weapon_g3sg1',\n",
       " 'ct_weapon_galilar',\n",
       " 't_weapon_galilar',\n",
       " 'ct_weapon_glock',\n",
       " 't_weapon_glock',\n",
       " 'ct_weapon_m249',\n",
       " 't_weapon_m249',\n",
       " 'ct_weapon_m4a1s',\n",
       " 't_weapon_m4a1s',\n",
       " 'ct_weapon_m4a4',\n",
       " 't_weapon_m4a4',\n",
       " 'ct_weapon_mac10',\n",
       " 't_weapon_mac10',\n",
       " 'ct_weapon_mag7',\n",
       " 't_weapon_mag7',\n",
       " 'ct_weapon_mp5sd',\n",
       " 't_weapon_mp5sd',\n",
       " 'ct_weapon_mp7',\n",
       " 't_weapon_mp7',\n",
       " 'ct_weapon_mp9',\n",
       " 't_weapon_mp9',\n",
       " 'ct_weapon_negev',\n",
       " 't_weapon_negev',\n",
       " 'ct_weapon_nova',\n",
       " 't_weapon_nova',\n",
       " 'ct_weapon_p90',\n",
       " 't_weapon_p90',\n",
       " 'ct_weapon_r8revolver',\n",
       " 't_weapon_r8revolver',\n",
       " 'ct_weapon_sawedoff',\n",
       " 't_weapon_sawedoff',\n",
       " 'ct_weapon_scar20',\n",
       " 't_weapon_scar20',\n",
       " 'ct_weapon_sg553',\n",
       " 't_weapon_sg553',\n",
       " 'ct_weapon_ssg08',\n",
       " 't_weapon_ssg08',\n",
       " 'ct_weapon_ump45',\n",
       " 't_weapon_ump45',\n",
       " 'ct_weapon_xm1014',\n",
       " 't_weapon_xm1014',\n",
       " 'ct_weapon_deagle',\n",
       " 't_weapon_deagle',\n",
       " 'ct_weapon_fiveseven',\n",
       " 't_weapon_fiveseven',\n",
       " 'ct_weapon_usps',\n",
       " 't_weapon_usps',\n",
       " 'ct_weapon_p250',\n",
       " 't_weapon_p250',\n",
       " 'ct_weapon_p2000',\n",
       " 't_weapon_p2000',\n",
       " 'ct_weapon_tec9',\n",
       " 't_weapon_tec9',\n",
       " 'ct_grenade_hegrenade',\n",
       " 't_grenade_hegrenade',\n",
       " 'ct_grenade_flashbang',\n",
       " 't_grenade_flashbang',\n",
       " 'ct_grenade_smokegrenade',\n",
       " 't_grenade_smokegrenade',\n",
       " 'ct_grenade_incendiarygrenade',\n",
       " 't_grenade_incendiarygrenade',\n",
       " 'ct_grenade_molotovgrenade',\n",
       " 't_grenade_molotovgrenade',\n",
       " 'ct_grenade_decoygrenade',\n",
       " 't_grenade_decoygrenade',\n",
       " 'round_winner']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "df_num = df.select_dtypes(include=numerics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ColumnDataSource' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6554ee81dcd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m  \u001b[1;34m'ct_grenade_decoygrenade'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m  \u001b[1;34m't_grenade_decoygrenade'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m  'round_winner'])\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-14dd173f367b>\u001b[0m in \u001b[0;36mhistotabs\u001b[1;34m(self, dataframe, features, log_scale, show_plot)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mhists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist_hover\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_plot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_plot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPanel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mhists\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-14dd173f367b>\u001b[0m in \u001b[0;36mhist_hover\u001b[1;34m(self, dataframe, column, bins, log_scale, show_plot)\u001b[0m\n\u001b[0;32m     45\u001b[0m                 hover_fill_alpha = 1.0, hover_fill_color = self.colors[1])\n\u001b[0;32m     46\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mColumnDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             plot = figure(plot_height = self.height, plot_width = self.width,\n\u001b[0;32m     49\u001b[0m                   \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Histogram of {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ColumnDataSource' is not defined"
     ]
    }
   ],
   "source": [
    "h.histotabs(df_num, ['time_left',\n",
    " 'ct_score',\n",
    " 't_score',\n",
    " 'ct_health',\n",
    " 't_health',\n",
    " 'ct_armor',\n",
    " 't_armor',\n",
    " 'ct_money',\n",
    " 't_money',\n",
    " 'ct_helmets',\n",
    " 't_helmets',\n",
    " 'ct_defuse_kits',\n",
    " 'ct_players_alive',\n",
    " 't_players_alive',\n",
    " 'ct_weapon_ak47',\n",
    " 't_weapon_ak47',\n",
    " 'ct_weapon_aug',\n",
    " 't_weapon_aug',\n",
    " 'ct_weapon_awp',\n",
    " 't_weapon_awp',\n",
    " 'ct_weapon_bizon',\n",
    " 't_weapon_bizon',\n",
    " 'ct_weapon_cz75auto',\n",
    " 't_weapon_cz75auto',\n",
    " 'ct_weapon_elite',\n",
    " 't_weapon_elite',\n",
    " 'ct_weapon_famas',\n",
    " 't_weapon_famas',\n",
    " 'ct_weapon_g3sg1',\n",
    " 't_weapon_g3sg1',\n",
    " 'ct_weapon_galilar',\n",
    " 't_weapon_galilar',\n",
    " 'ct_weapon_glock',\n",
    " 't_weapon_glock',\n",
    " 'ct_weapon_m249',\n",
    " 't_weapon_m249',\n",
    " 'ct_weapon_m4a1s',\n",
    " 't_weapon_m4a1s',\n",
    " 'ct_weapon_m4a4',\n",
    " 't_weapon_m4a4',\n",
    " 'ct_weapon_mac10',\n",
    " 't_weapon_mac10',\n",
    " 'ct_weapon_mag7',\n",
    " 't_weapon_mag7',\n",
    " 'ct_weapon_mp5sd',\n",
    " 't_weapon_mp5sd',\n",
    " 'ct_weapon_mp7',\n",
    " 't_weapon_mp7',\n",
    " 'ct_weapon_mp9',\n",
    " 't_weapon_mp9',\n",
    " 'ct_weapon_negev',\n",
    " 't_weapon_negev',\n",
    " 'ct_weapon_nova',\n",
    " 't_weapon_nova',\n",
    " 'ct_weapon_p90',\n",
    " 't_weapon_p90',\n",
    " 'ct_weapon_r8revolver',\n",
    " 't_weapon_r8revolver',\n",
    " 'ct_weapon_sawedoff',\n",
    " 't_weapon_sawedoff',\n",
    " 'ct_weapon_scar20',\n",
    " 't_weapon_scar20',\n",
    " 'ct_weapon_sg553',\n",
    " 't_weapon_sg553',\n",
    " 'ct_weapon_ssg08',\n",
    " 't_weapon_ssg08',\n",
    " 'ct_weapon_ump45',\n",
    " 't_weapon_ump45',\n",
    " 'ct_weapon_xm1014',\n",
    " 't_weapon_xm1014',\n",
    " 'ct_weapon_deagle',\n",
    " 't_weapon_deagle',\n",
    " 'ct_weapon_fiveseven',\n",
    " 't_weapon_fiveseven',\n",
    " 'ct_weapon_usps',\n",
    " 't_weapon_usps',\n",
    " 'ct_weapon_p250',\n",
    " 't_weapon_p250',\n",
    " 'ct_weapon_p2000',\n",
    " 't_weapon_p2000',\n",
    " 'ct_weapon_tec9',\n",
    " 't_weapon_tec9',\n",
    " 'ct_grenade_hegrenade',\n",
    " 't_grenade_hegrenade',\n",
    " 'ct_grenade_flashbang',\n",
    " 't_grenade_flashbang',\n",
    " 'ct_grenade_smokegrenade',\n",
    " 't_grenade_smokegrenade',\n",
    " 'ct_grenade_incendiarygrenade',\n",
    " 't_grenade_incendiarygrenade',\n",
    " 'ct_grenade_molotovgrenade',\n",
    " 't_grenade_molotovgrenade',\n",
    " 'ct_grenade_decoygrenade',\n",
    " 't_grenade_decoygrenade',\n",
    " 'round_winner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "text",
    "id": "ANI-KEZYZDTe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x184e335c948>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " import seaborn as sns\n",
    " \n",
    " sns.catplot(x=\"map\", kind=\"count\", data=df, height=5, aspect=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x184e0803148>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1156.25x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(x=\"map\", hue=\"round_winner\", kind=\"count\", data=df, height=5, aspect=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two most played maps, de_inferno and de_dust2, Terrorist team wins more games than Counter-Terrorist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "id": "0zjzzwTdZDTe",
    "outputId": "68e2b047-6b03-4ffb-a78e-dd12ab3fd89e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x184e36fde88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1156.25x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(x=\"ct_defuse_kits\", hue=\"round_winner\", kind=\"count\", data=df, height=5, aspect=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having more than 1 defuse kit in the Counter-Terrotist team makes CT win more rounds than T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x18480394208>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1156.25x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(x=\"bomb_planted\", hue=\"round_winner\", kind=\"count\", data=df, height=5, aspect=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Terrorists plant the bomb their win ratio is much better, but planting the bomb doesn´t happen frequetly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "FgcxzXqqZDTf"
   },
   "source": [
    "# Baseline <font color=red>CKALIB AND TOMAS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the CS:GO Kaggle competition is now closed, so we must evaluate your results using an alternative method. All the teams will have to include this snippet of code in order to read the data, and split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = 'round_winner'\n",
    "features = [column for column in data.columns if column != target]\n",
    "\n",
    "X, X_val, y, y_val = train_test_split(\n",
    "    data[features],\n",
    "    data[target],\n",
    "    test_size=0.3,\n",
    "    random_state=1,\n",
    "    stratify=data[target])\n",
    "\n",
    "print(X.shape)\n",
    "print(X_val.shape)\n",
    "print(y.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the validation set will be used to obtain the FINAL score of your model, not the intermediate validation scores that will help you to determine whether your model is overfitting. So, you will need to further split your data (X and y) sets into training and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "772U0c25ZDTf"
   },
   "source": [
    "I will build a method to evaluate a basic logistic regression over a split of the data set using cross validation.\n",
    "\n",
    "The method requires that you send it the independent variables (X) and the dependent variables (y) as a pandas DataFrame. We can access the dataframe inside the `Dataset` by calling, either, `.features` or `.target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRjOX_hxZDTf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, matthews_corrcoef\n",
    "\n",
    "def evaluate_LogReg(X: pd.DataFrame, y: pd.DataFrame,\n",
    "                    metric: str='F1',\n",
    "                    seed: int=123,\n",
    "                    printout: bool=False):\n",
    "    \"\"\"\n",
    "    Fits a logistic regression with the training set, and evaluates it with \n",
    "    the test set, using Accuracy, F1, Recall or MCC metrics.\n",
    "\n",
    "    Params\n",
    "      X: A Pandas DataFrame with all the values to be used to fit the logit\n",
    "      y: The dependent variable.\n",
    "      metric: The metric to be returned\n",
    "      printout: True/False indicating whether you want to print results\n",
    "\n",
    "    \"\"\"\n",
    "    # Split the data is the first thing to do\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2)\n",
    "\n",
    "    lr = LogisticRegression().fit(X_train, y_train)\n",
    "    y_hat = lr.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_hat)\n",
    "    F1 = f1_score(y_test, y_hat)\n",
    "    recall = recall_score(y_test, y_hat)\n",
    "    MCC = matthews_corrcoef(y_test, y_hat)\n",
    "    \n",
    "    if printout is True:\n",
    "        print('Accuracy:', accuracy)\n",
    "        print('F1:', F1)\n",
    "        print('Recall:', recall)\n",
    "        print('MCC:', MCC)\n",
    "    \n",
    "    return eval(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5az2g6C8nMpA"
   },
   "source": [
    "Let's run the **baseline**. We will simply call our method with the numerical features only, simply to see what is the expected F1 metric. Ideally, we should call the evaluation method with all our features, but to do so, we must convert our categorical features into numbers. For baselining, we can start by simply applying a **onehot** encoding to them to see what is the result.\n",
    "\n",
    "# Baseline with numerical features only\n",
    "\n",
    "To start with, let's simply use numericals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "vBHokEQxZDTg",
    "outputId": "1b08918e-2888-4a5e-cf8d-82e9a52fcf7b"
   },
   "outputs": [],
   "source": [
    "evaluate_LogReg(data.numerical, data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmsclXBEnscy"
   },
   "source": [
    "Now, we will try to onehot our categoricals to apply the LogisticRegression to all our features. We will use a copy of the dataset since we do not want to premanently encode our data with that scheme.\n",
    "\n",
    "# Basline with all features (onehot)\n",
    "\n",
    "I will use Dataset method `onehot_encode` but you can use Pandas `get_dummies` or the one provided by `scikit learn`.\n",
    "\n",
    "as you can see, the result is a Dataset with 30 numerical variables, instead of the original combination of 21 numericals + 4 categoricals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "Mang7SPboGZr",
    "outputId": "165a78d5-6e21-4a43-9d7a-753318efe2d5"
   },
   "outputs": [],
   "source": [
    "# Get a copy of data\n",
    "from copy import deepcopy\n",
    "data_copy = deepcopy(data)\n",
    "\n",
    "# Onehot\n",
    "data_copy.onehot_encode()\n",
    "data_copy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "1ntdLStiowkc",
    "outputId": "031797ab-3005-4321-995b-1c7fbaa3d1c9"
   },
   "outputs": [],
   "source": [
    "# Call the evaluation method with all data in the Dataset.\n",
    "evaluate_LogReg(data_copy.features, data_copy.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zLG-6Ufvpusd"
   },
   "source": [
    "It seems that adding the categorical features produces a different result. However, if you remember the rules of Logit we must avoid co-linearity, variables which are not completely independent or even not normal distributions.\n",
    "\n",
    "We will simplify later on the evaluation by using a simpler model.\n",
    "\n",
    "## Problems with this baseline\n",
    "\n",
    "However, we cannot evaluate a model like I proposed before. To demostrate what is the problem, I will call the same method 100 times, storing its result each time in a list, to later plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "0ciPAhxucDhq",
    "outputId": "fdab2d3e-157f-4e08-b814-6a8a13567ff9"
   },
   "outputs": [],
   "source": [
    "my_list = []\n",
    "for _ in range(20):\n",
    "    result = evaluate_LogReg(data_copy.features, data_copy.target)\n",
    "    my_list.append(result)\n",
    "plt.scatter(range(20), my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A6T-V9nNcU43"
   },
   "source": [
    "We've obtained 100 different values for the evaluation of the baseline. Which one is the good one? The highest value? The lowest? The average? The median? ....\n",
    "\n",
    "None of them... this plot simply illustrates that if you split your dataset 100 times, you train your model with different portions of the data 100 times, producing 100 different results.\n",
    "\n",
    "To avoid this _variance_ in results, we want to estimate what will be the expected result in test. To achieve that we use **cross validation**. The idea is to split the data a number of times but use the average test score obtained from all of them as a reasonable estimation of what to expect from this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mV-UDxECbjMN"
   },
   "source": [
    "## Cross Validation \n",
    "\n",
    "To evaluate our model in a slightly simpler way, let's use this method: `cv_classification`. It accepts a Dataset as input, and an `estimator`, which is simply the model I want to use.\n",
    "\n",
    "Internally, it will repeat a number of times (`num_iterations`) the following steps:\n",
    "\n",
    "  1. Randomly split the data set into training and test\n",
    "  2. Fit the estimator with the training portion of the data\n",
    "  3. Obtain a CV metric in training for this model\n",
    "  4. Obtain a metric in test for the predictions obtained with the model just fitted in step 2\n",
    "  5. Save the scores in an array to later return all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OPM3ncUf8N0Y"
   },
   "outputs": [],
   "source": [
    "def cv_classification(my_dataset, estimator, num_iterations=20, verbose=True):\n",
    "    train_scores=[]\n",
    "    test_scores=[]\n",
    "    for i in range(num_iterations):\n",
    "        X, y = my_dataset.split(seed=23*i ^ 2)\n",
    "\n",
    "        # We want to call a method whose name is the argument to this func.\n",
    "        estimator.fit(X.train, y.train)\n",
    "        train_metric = cross_val_score(estimator, X.train, y.train, scoring='f1')\n",
    "        train_scores.append(np.median(train_metric))\n",
    "\n",
    "        test_score = f1_score(y.test, estimator.predict(X.test))\n",
    "        test_scores.append(test_score)\n",
    "\n",
    "    if verbose:\n",
    "        print('Training median F1: {:.4f} +/- {:.2f}%'.format(\n",
    "            np.median(train_scores), np.std(train_scores)*100.))\n",
    "        print('Test F1: {:.4f} +/- {:.2f}%'.format(\n",
    "            np.median(test_scores), np.std(test_scores)*100.))\n",
    "\n",
    "    return train_scores, test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LG6Q2T1llqiI"
   },
   "source": [
    "So, if we decide to use the method above to score our Linear Regression model using cross validation (20 iterations), what we have to do is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "P5BC2e96QCuP",
    "outputId": "4859ef66-37df-45a8-b848-5320dd200cb5"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "train_lr_scores, test_lr_scores = cv_classification(data_copy, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lX4zp5yzmH17"
   },
   "source": [
    "The interpretation from these training and test scores is the following:\n",
    "\n",
    "* Our model, in training, is capable of achieving a 56.17% F1 score, with a variance of 3.1%. This is useful to check if our training is getting very high results (overfitting) or very high variance (data contains too much noise).\n",
    "* Our model is expected to throw a F1 of 54% (+/- 4.5%) when predicting over unseen data.\n",
    "* Since performance test is a bit below training performance, that means that we're not overfitting, and that our model is generalizing correctly. If test performance were better than training performance, that's an indication that your training can be improved by tuning a bit more your parameters (too much bias).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tznHn84-boxq"
   },
   "source": [
    "## Baseline with decision tree\n",
    "\n",
    "To make things a bit simpler in our baseline, we will use a decision tree. Spending too much time cleaning our data to fit normal distributions and finding co-linearity could not be worthy in the end. So, let's try to produce a quick baseline with a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "JjmCmTJtJmAT",
    "outputId": "29cc1778-4644-43e3-b3fd-439714f8ae31"
   },
   "outputs": [],
   "source": [
    "baseline_tree = DecisionTreeClassifier(random_state=123)\n",
    "base_train, base_test = cv_classification(data_copy, baseline_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "eDAbh9elZDTh"
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "We'll try a conservative approach in our feature engineering process. Instead of playing too much attention to filtering, we'll try only find highly correlated features, to focus on creating new features.\n",
    "\n",
    "I'm using the original `data`. I will discard the onehot version, as I want to play with the original features to later perform the best possible encoding (not only onehot encoding).\n",
    "\n",
    "## Catboost encoding vs. Target encoding\n",
    "\n",
    "We're going to compare what is the effect of encoding with Catboost or Target encoding vs. the onehot encoded proposed above for the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "UJtTZhDtZDTh"
   },
   "outputs": [],
   "source": [
    "# Uncomment this line if you haven't installed caatboos (categorical_encoders) already\n",
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple `fit_transform` our data using the proposed encoder, and then:\n",
    "  - we create another dataset (since we don't want to destroy the original one)\n",
    "  - we add the new numerically encoded features proposed by Catboost\n",
    "  - we remove the old categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Catboost encoder, and \"fit\" + \"transform\" our data.\n",
    "catboost_enc = ce.CatBoostEncoder(cols=data.categorical_features)\n",
    "new_columns = catboost_enc.fit_transform(\n",
    "    data.categorical, data.target).add_suffix('_cb')\n",
    "\n",
    "# Create another Dataset with my original features\n",
    "data_cb = Dataset.from_dataframe(data.all)\n",
    "data_cb.set_target('band_type')\n",
    "# ... and I add the newly encoded columns, removing the categorical ones.\n",
    "data_cb.add_columns(new_columns).drop_columns(data_cb.categorical_features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train a decission tree classifier with the new data to see if we're improving a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_tree = DecisionTreeClassifier(random_state=666)\n",
    "cb_train, cb_test = cv_classification(data_cb, catboost_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to try the TargetEncoder. Same steps as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Catboost encoder, and \"fit\" + \"transform\" our data.\n",
    "target_enc = ce.TargetEncoder(cols=data.categorical_features)\n",
    "new_columns = target_enc.fit_transform(\n",
    "    data.categorical, data.target).add_suffix('_tg')\n",
    "\n",
    "# Create another Dataset with my original features\n",
    "data_tg = Dataset.from_dataframe(data.all)\n",
    "data_tg.set_target('band_type')\n",
    "# ... and I add the newly encoded columns, removing the categorical ones.\n",
    "data_tg.add_columns(new_columns).drop_columns(data_tg.categorical_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tree = DecisionTreeClassifier(random_state=666)\n",
    "tg_train, tg_test = cv_classification(data_tg, target_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot the three results obtained so far.\n",
    "x_pos = range(3)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, \n",
    "       [np.median(base_test), np.median(cb_test), np.median(tg_test)], \n",
    "       yerr=[np.std(base_test), np.std(cb_test), np.std(tg_test)], \n",
    "       align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_ylabel('Test F1 score')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['baseline','catboost','target'])\n",
    "ax.set_ylim([0.50, 0.75])\n",
    "ax.set_title('Test F1 on a decision tree with diff. encodings')\n",
    "ax.yaxis.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that there's no big difference but, a priori, this experiment suggests that catboost encoding is giving us some advantage.\n",
    "\n",
    "We will keep that dataset (without categorical variables, to continue deciding on how to move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_dataframe(data_cb.all)\n",
    "data.set_target('band_type');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Feature Selection\n",
    "\n",
    "We cannot explore deep feature synthesis, as it is required a lot of knowledge about the problem domain that we don't have for this dataset. GPlearn is a good alternative, but let's filter out some attributes that might be too much correlated or not leading to good performance.\n",
    "\n",
    "### Filtering\n",
    "\n",
    "When plotting the covariance matrix we can see a candidate group of features that might present high correlation and could be discarded using wrapper methods (blade_pressure, was, varnish_pct, paper_mill_local_cb, proof_cut, roller_durometer and grain_screen_cb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot_covariance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any filtering decision should be based on a relevant gain in our model. So, the steps to be followed must be:\n",
    "  1. select features to be removed\n",
    "  2. evaluate model without them\n",
    "  3. if new performance (in test) is better, then continue\n",
    "  4. otherwise, keep the features.\n",
    "  \n",
    "### Recursive Feature Elimination\n",
    "\n",
    "Let's start by using Recursive Feature Elimination from sklearn. It's a sophisticated implementation of stepwise selection, but with recursive decisions instead of single-step decisions. The parameters we need to have in order to use it are:\n",
    "  - A model –our decision tree, in our case\n",
    "  - The number of features we want to be finally selected\n",
    "  - The `step` which is the nr. of features to be removed at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that calls RFE and returns the best features found\n",
    "def rfe(data, num_features):\n",
    "    estimator = DecisionTreeClassifier(random_state=123)\n",
    "    selector = RFE(estimator, num_features, step=1)\n",
    "    selector = selector.fit(data.features, data.target)\n",
    "    return selector.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code will evaluate a range of possible subsets of features, from 5 to all features included. At each iteration, it will keep the test score to later plot it, and see what is the best strategy to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the range of features in our dataset and capture results.\n",
    "rfe_test_scores = pd.DataFrame()\n",
    "min_range = 5\n",
    "max_range = len(data.feature_names)\n",
    "rfe_tree = DecisionTreeClassifier(random_state=666)\n",
    "\n",
    "# This is a loop over all possible values I want to evaluate\n",
    "for num_features in range(min_range, max_range):\n",
    "    # Obtain the best features\n",
    "    best_features = rfe(data, num_features)\n",
    "    # Build a new dataset ONLY with best features selected.\n",
    "    data_rfe = Dataset.from_dataframe(\n",
    "        pd.concat(\n",
    "            [data.features[data.features.columns[best_features]], data.target], axis=1\n",
    "        )\n",
    "    )\n",
    "    data_rfe.set_target('band_type')\n",
    "\n",
    "    # Evaluate (CV) what is the F1 of this dataset\n",
    "    _, test_f1 = cv_classification(data_rfe, rfe_tree, verbose=False)\n",
    "    \n",
    "    # Keep results to later plot them\n",
    "    rfe_test_scores = rfe_test_scores.append(pd.Series(test_f1), ignore_index=True)\n",
    "    del(data_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x_pos = range(min_range, max_range)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "ax.boxplot(rfe_test_scores)\n",
    "ax.set_ylabel('Test F1 score')\n",
    "ax.set_xticks(range(1, len(x_pos)))\n",
    "ax.set_xticklabels(x_pos)\n",
    "# ax.set_ylim([0.50, 0.75])\n",
    "ax.set_title('Test F1 with varying nr. of features')\n",
    "ax.yaxis.grid(True, color=\"gray\", linestyle='--', linewidth=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my experiment, 10 features is the best option. It also presents a not very high variance. So, now I call RFE asking it to return me 10 features and continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = rfe(data, 10)\n",
    "data_rfe = Dataset.from_dataframe(\n",
    "    pd.concat(\n",
    "        [data.features[data.features.columns[best_features]], data.target], axis=1\n",
    "    )\n",
    ")\n",
    "data_rfe.set_target('band_type')\n",
    "data_rfe.features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that this selection keeps the performance observed in the previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_train, rfe_test = cv_classification(data_rfe, rfe_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_dataframe(data_rfe.all)\n",
    "data.set_target('band_type');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Compare the results obtained using RFE with Relief algorithm or stepwise selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "xY_MrLaKZDTi"
   },
   "source": [
    "# Evaluation and Validation\n",
    "\n",
    "We have 10 features, and out results are much better than those obtained in the baseline. What's next then? \n",
    "\n",
    "First topic to explore is how to reduce the variance in test, since we've a large number as a result of applyin the CV process in `cv_classification()`. This type of issues is normally addressed using **bootstrapping**. You've an example in [this notebook](https://github.com/renero/class_notebooks/blob/master/Evaluation%20and%20Validation.ipynb).\n",
    "\n",
    "We should also take a deeper **look into our classification results** to be sure that our metric is capturing well positive and negative cases. Let's see how to access the different classifications obtained in our test set, to see from there if we can improve our metric.\n",
    "\n",
    "We must also experiment with **different metrics**: AUC, ROC, double density plots and confusion matrices.\n",
    "\n",
    "Another suggestion is that we will tune the decision tree by, for example **pruning** it with CV and CCP.\n",
    "\n",
    "Finally, compare our best single-decision-tree with **bagging** and **random forests**, for instance. We must do it, NOT only changing the model name before calling the `fit()` method. We must also perform hyperparameter tuning, and we'll see how is that done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "YAFWzcGUZDTj"
   },
   "source": [
    "## Bootstrapping\n",
    "\n",
    "Bootstrapping is a technique that can be used to estimate population statistics by repeatedly sampling with replacement and measuring. But this technique can also be used to perform model validation in a very similar way to cross-validation: we will evaluate our model with bootstrapped samples of our population.\n",
    "\n",
    "Out method `bootstrap_split()` will simply accept all the features, and the target variable to produce two sets: training and test. This is the same idea we use when we split a dataset, but instead of taking a random number of samples out of the original data, _bootstrapping_ will perform sampling with repetition. That is the part that the method `resample()` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_split(X, y, seed, training_size=0.8):\n",
    "    train_num_samples = X.shape[0]\n",
    "    X_train, y_train = resample(X, y, replace=True, \n",
    "                                n_samples=int(train_num_samples * 0.8),\n",
    "                                random_state=seed)\n",
    "\n",
    "    # Take the indices present in the training samples\n",
    "    indices_in_training = X_train.index.to_list()\n",
    "\n",
    "    # Those, NOT in training are, go to the test set.\n",
    "    X_test = X[~X.index.isin(indices_in_training)]\n",
    "    y_test = y[~y.index.isin(indices_in_training)]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've our splitting method, it's time to repeat the process of splitting and evaluating to have a decent idea of what can we expect from our model in test. These results will, ideally, produce less variance in its estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_scores = []  # I will use this to keep every score produced in test.\n",
    "bs_tree = DecisionTreeClassifier(random_state=666)\n",
    "\n",
    "for i in range(100):\n",
    "    X_train, y_train, X_test, y_test = bootstrap_split(data.features, data.target, seed=i*23)\n",
    "    bs_tree.fit(X_train, y_train)\n",
    "    \n",
    "    train_metric = cross_val_score(bs_tree, X_train, y_train, scoring='f1')\n",
    "    test_score = f1_score(y_test, bs_tree.predict(X_test))\n",
    "    bs_scores.append(test_score)\n",
    "\n",
    "print(\"F1 (bootstrapping) in test: %0.4f (+/- %0.2f)\" % (np.median(bs_scores), np.std(bs_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the variance (std) in our metric is significantly lower... which means that we're a bit more confident about our model's performance, by using this metric.\n",
    "\n",
    "To really evaluate which one is giving us the closest estimate of what will happen when the model will be used with unseen data, let's keep a portion of the data out of the entire process (validation subset). Then, let's use it at the end to perform a single evaluation to see if it falls closer to the CV estimate, or to the bootstrap estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.split(seed=666)\n",
    "val_tree = DecisionTreeClassifier(random_state=666)\n",
    "val_tree.fit(X.train, y.train)\n",
    "\n",
    "f1_score(y.test, val_tree.predict(X.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results gives us the idea that our models is producing a result very close to that predicted by our bootstrapping validation technique (and not too far either from what we got using CV).\n",
    "\n",
    "### EXERCISE\n",
    "\n",
    "Compare the results obtained with bootstrapping vs. cross-validation using different number of iterations to boxplot those results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification results\n",
    "\n",
    "Let's plot the decision tree to check some useful information that we can find in the plot. \n",
    "\n",
    "  - Both categories (in our binomial classification problem) are represented with the two colors: the more intense the color, the higher the purity of the classification. Lighter nodes have Gini indices close to 0.5 (maximum impurity).\n",
    "  - Every node shows you the nr. of samples that felt into each of the casses (value = [class1, class2])\n",
    "  - The decision is also represented, so you can also see what're the features that play more important roles in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "my_tree = DecisionTreeClassifier(random_state=666, max_depth=3)\n",
    "my_tree.fit(X.train, y.train)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 7))\n",
    "tree.plot_tree(my_tree, feature_names=data.feature_names, \n",
    "               node_ids=True, filled=True, fontsize=8, max_depth=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know how to hack what node in the tree is producing a given prediction, we need to ask in stackoverflow, where [this response](https://stackoverflow.com/a/42227468) is one that I like. We need some code here, but it is sort of obscure. Important takeaway is that this method allows you to know what are the Gini values for each classification performed on a test set (`X_test`). You only need to pass the fitted tree (_estimator_) and the test set to obtain in return an array with all the Gini values obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impurity(estimator, X_test, verbose=False):\n",
    "    impurity = []\n",
    "    leave_id = estimator.apply(X_test)\n",
    "    node_indicator = estimator.decision_path(X_test)\n",
    "    feature = estimator.tree_.feature\n",
    "    threshold = estimator.tree_.threshold\n",
    "\n",
    "    for sample_id in range(X_test.shape[0]):\n",
    "        node_index = node_indicator.indices[\n",
    "            node_indicator.indptr[sample_id]:\n",
    "            node_indicator.indptr[sample_id + 1]]\n",
    "        leave = estimator.tree_.__getstate__()['nodes'][leave_id[sample_id]]\n",
    "        if verbose:\n",
    "            print(f\"sample {sample_id:>3d} predicted at leave ID #{leave_id[sample_id]:>2d}. \", end=\"\")\n",
    "            print(f\"Impurity (Gini): {leave[4]:.4f}\")\n",
    "        impurity.append(leave[4])\n",
    "    \n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what our predictions look like, we can plot the histogram of the Gini values. Ideally we'd see lots of values close to zero, and not so many close to 0.5. But, reality shows a different picture. In our case, our predictions are not very good, which means that our features are improving prediction accuracy over the baseline, but maybe they're not very good.\n",
    "\n",
    "What is the solution to this? Increase the depth of the tree, or increase the complexity of the tree (baggin, boosting, random forests, ...), to name a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impurities = get_impurity(my_tree, X_test)\n",
    "\n",
    "plt.hist(impurities);\n",
    "plt.title('Distribution of impurities in test')\n",
    "plt.xlabel('Gini')\n",
    "plt.ylabel('Numer of samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE\n",
    "\n",
    "Try to increase the max depth of the tree to values higher than \"3\", and check what the Gini values look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "In this section, we experiment with some other metrics like AUC, ROC or confusion matrix. Once you have trained (fitted) a decision tree, and you have your training and test sets, you can compute any metric you like. I will leave this points as an exercise to you. \n",
    "\n",
    "Hint: Look at [this section of the sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html) to know more about using different metrics.\n",
    "\n",
    "### EXERCISE\n",
    "\n",
    "Compute the confusion matrix, the AUX and plot the ROC curve from our classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "\n",
    "Another validation on trees is simply to prune the tree at optimal depth, to obtain the maximum score, and better generalization. The value \"3\" that we've used before is too low, but if don't specify anything, the tree will grow too much.\n",
    "\n",
    "HINT: Look at [this tutorial on sklearn page](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html) that shows you how to use cost-complexity pruning and cross-validation (alpha) pruning.\n",
    "\n",
    "### EXERCISE\n",
    "\n",
    "Determine what is the optimal depth for the tree, using cost complexity pruning (as explained in the tutorial suggested before), and cross validation.\n",
    "\n",
    "To run the CV pruning, you simply must iterate over different values of `max_depth` and evaluate your fitted tree using `cv_validate()` to later decide what is the value of depth that provides the optimal depth for the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Random Forests\n",
    "\n",
    "As a final step, we can concentrate on whether the model selected is the best possible option. We can compare our results with a single tree against the option of using Bagging trees or Random Forest. I will also leave this as an exercise to you. You can find all the information you need in my Trees notebook in [github](https://github.com/renero/class_notebooks/blob/master/Tree%20based%20methods.ipynb), or in the [sklearn page](https://scikit-learn.org/stable/modules/ensemble.html).\n",
    "\n",
    "### EXERCISE\n",
    "\n",
    "Fit and evaluate models based on Bagging and Random Forests (optional, Boosting trees), and compare the score obtained in test, with the single tree model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practice on Feature Engineering and Evaluation Monday.ipynb",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
